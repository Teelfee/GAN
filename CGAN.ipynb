{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teelfee/GAN_learning/blob/development/CGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FW916i9taDkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "下载mnist\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uXwxG4kFVjZ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import zipfile\n",
        "import argparse\n",
        "import subprocess\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers.python.layers import batch_norm, variance_scaling_initializer\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.contrib.layers.python.layers import xavier_initializer\n",
        "import cv2\n",
        "\n",
        "def download(url, dirpath):\n",
        "\n",
        "    filename = url.split('/')[-1]\n",
        "    filepath = os.path.join(dirpath, filename)\n",
        "    u = urllib.request.urlopen(url)\n",
        "    f = open(filepath, 'wb')\n",
        "    filesize = int(u.headers[\"Content-Length\"])\n",
        "    print(\"Downloading: %s Bytes: %s\" % (filename, filesize))\n",
        "\n",
        "    downloaded = 0\n",
        "    block_sz = 8192\n",
        "    status_width = 70\n",
        "    while True:\n",
        "        buf = u.read(block_sz)\n",
        "        if not buf:\n",
        "            print('')\n",
        "            break\n",
        "        else:\n",
        "            print('', end='\\r')\n",
        "        downloaded += len(buf)\n",
        "        f.write(buf)\n",
        "        status = ((\"[%-\" + str(status_width + 1) + \"s] %3.2f%%\") %\n",
        "            ('=' * int(float(downloaded) / filesize * status_width) + '>', downloaded * 100. / filesize))\n",
        "        print(status, end='')\n",
        "        sys.stdout.flush()\n",
        "    f.close()\n",
        "    return filepath\n",
        "\n",
        "def unzip(filepath):\n",
        "    print(\"Extracting: \" + filepath)\n",
        "    dirpath = os.path.dirname(filepath)\n",
        "    with zipfile.ZipFile(filepath) as zf:\n",
        "        zf.extractall(dirpath)\n",
        "    os.remove(filepath)\n",
        "\n",
        "\n",
        "def download_mnist(dirpath):\n",
        "    data_dir = os.path.join(dirpath, 'mnist')\n",
        "    if os.path.exists(data_dir):\n",
        "        print('Found MNIST - skip')\n",
        "        return\n",
        "    else:\n",
        "        os.mkdir(data_dir)\n",
        "    url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "    file_names = ['train-images-idx3-ubyte.gz','train-labels-idx1-ubyte.gz','t10k-images-idx3-ubyte.gz','t10k-labels-idx1-ubyte.gz']\n",
        "    for file_name in file_names:\n",
        "        url = (url_base+file_name).format(**locals())\n",
        "        print(url)\n",
        "        out_path = os.path.join(data_dir,file_name)\n",
        "        cmd = ['curl', url, '-o', out_path]\n",
        "        print('Downloading ', file_name)\n",
        "        subprocess.call(cmd)\n",
        "        cmd = ['gzip', '-d', out_path]\n",
        "        print('Decompressing ', file_name)\n",
        "        subprocess.call(cmd)\n",
        "\n",
        "def prepare_data_dir(path = './data'):\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     args = parser.parse_args()\n",
        "#     prepare_data_dir()\n",
        "\n",
        "#     if 'celebA' in args.datasets:\n",
        "#         download_celeb_a('./data')\n",
        "#     if 'lsun' in args.datasets:\n",
        "#         download_lsun('./data')\n",
        "#     if 'mnist' in args.datasets:\n",
        "#         download_mnist('./data')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gndNYf2bdPMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "543beb59-5feb-4e04-ce39-8d76153b48b6"
      },
      "cell_type": "code",
      "source": [
        "prepare_data_dir()\n",
        "download_mnist('./data')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found MNIST - skip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7o3ZrYXzmnMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "定义一些工具"
      ]
    },
    {
      "metadata": {
        "id": "MTtR2nlOwym2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the implements of leakyRelu\n",
        "def lrelu(x , alpha = 0.2 , name=\"LeakyReLU\"):\n",
        "    return tf.maximum(x , alpha*x)\n",
        "\n",
        "def conv2d(input_, output_dim,\n",
        "           k_h=3, k_w=3, d_h=2, d_w=2,\n",
        "           name=\"conv2d\"):\n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
        "                            initializer= variance_scaling_initializer())\n",
        "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
        "\n",
        "        return conv, w\n",
        "\n",
        "def de_conv(input_, output_shape,\n",
        "             k_h=3, k_w=3, d_h=2, d_w=2, stddev=0.02, name=\"deconv2d\", \n",
        "             with_w=False, initializer = variance_scaling_initializer()):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        # filter : [height, width, output_channels, in_channels]\n",
        "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
        "                            initializer = initializer)\n",
        "        try:\n",
        "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n",
        "                                            strides=[1, d_h, d_w, 1])\n",
        "        # Support for verisons of TensorFlow before 0.7.0\n",
        "        except AttributeError:\n",
        "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
        "                                    strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
        "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
        "\n",
        "        if with_w:\n",
        "            return deconv, w, biases\n",
        "        else:\n",
        "            return deconv\n",
        "\n",
        "def fully_connect(input_, output_size, scope=None, with_w=False, \n",
        "                  initializer = variance_scaling_initializer()):\n",
        "\n",
        "  shape = input_.get_shape().as_list()\n",
        "\n",
        "  with tf.variable_scope(scope or \"Linear\"):\n",
        "\n",
        "    matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
        "                 initializer = initializer)\n",
        "    bias = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(0.0))\n",
        "    if with_w:\n",
        "      return tf.matmul(input_, matrix) + bias, matrix, bias\n",
        "    else:\n",
        "      return tf.matmul(input_, matrix) + bias\n",
        "\n",
        "def conv_cond_concat(x, y):\n",
        "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
        "    x_shapes = x.get_shape()\n",
        "    y_shapes = y.get_shape()\n",
        "\n",
        "    return tf.concat([x , y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2] , y_shapes[3]])], 3)\n",
        "\n",
        "def batch_normal(input , scope=\"scope\" , reuse=False):\n",
        "    return batch_norm(input , epsilon=1e-5, decay=0.9 , scale=True, scope=scope , reuse = reuse , updates_collections=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45rc-MOp0jI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Mnist(object):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.dataname = \"Mnist\"\n",
        "        self.dims = 28*28\n",
        "        self.shape = [28 , 28 , 1]\n",
        "        self.image_size = 28\n",
        "        self.data, self.data_y = self.load_mnist()\n",
        "\n",
        "    def load_mnist(self):\n",
        "\n",
        "        data_dir = os.path.join(\"./data\", \"mnist\")\n",
        "        fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n",
        "        loaded = np.fromfile(file=fd , dtype=np.uint8)\n",
        "        trX = loaded[16:].reshape((60000, 28 , 28 ,  1)).astype(np.float)\n",
        "\n",
        "        fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        trY = loaded[8:].reshape((60000)).astype(np.float)\n",
        "\n",
        "        fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teX = loaded[16:].reshape((10000, 28 , 28 , 1)).astype(np.float)\n",
        "\n",
        "        fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n",
        "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((10000)).astype(np.float)\n",
        "\n",
        "        trY = np.asarray(trY)\n",
        "        teY = np.asarray(teY)\n",
        "\n",
        "        X = np.concatenate((trX, teX), axis=0)\n",
        "        y = np.concatenate((trY, teY), axis=0)\n",
        "\n",
        "        seed = 547\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(X)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(y)\n",
        "\n",
        "        #convert label to one-hot\n",
        "\n",
        "        y_vec = np.zeros((len(y), 10), dtype=np.float)\n",
        "        for i, label in enumerate(y):\n",
        "            y_vec[i, int(y[i])] = 1.0\n",
        "\n",
        "        return X / 255., y_vec\n",
        "\n",
        "    def getNext_batch(self, iter_num=0, batch_size=64):\n",
        "\n",
        "        ro_num = len(self.data) / batch_size - 1\n",
        "\n",
        "        if iter_num % ro_num == 0:\n",
        "\n",
        "            length = len(self.data)\n",
        "            perm = np.arange(length)\n",
        "            np.random.shuffle(perm)\n",
        "            self.data = np.array(self.data)\n",
        "            self.data = self.data[perm]\n",
        "            self.data_y = np.array(self.data_y)\n",
        "            self.data_y = self.data_y[perm]\n",
        "\n",
        "        return self.data[int(iter_num % ro_num) * batch_size: int(iter_num% ro_num + 1) * batch_size] \\\n",
        "            , self.data_y[int(iter_num % ro_num) * batch_size: int(iter_num%ro_num + 1) * batch_size]\n",
        "\n",
        "\n",
        "def get_image(image_path , is_grayscale = False):\n",
        "    return np.array(inverse_transform(imread(image_path, is_grayscale)))\n",
        "\n",
        "\n",
        "def save_images(images , size , image_path):\n",
        "    return imsave(inverse_transform(images) , size , image_path)\n",
        "\n",
        "def imread(path, is_grayscale = False):\n",
        "    if (is_grayscale):\n",
        "        return scipy.misc.imread(path, flatten = True).astype(np.float)\n",
        "    else:\n",
        "        return scipy.misc.imread(path).astype(np.float)\n",
        "\n",
        "def imsave(images , size , path):\n",
        "    return scipy.misc.imsave(path , merge(images , size))\n",
        "\n",
        "def merge(images , size):\n",
        "    h , w = images.shape[1] , images.shape[2]\n",
        "    img = np.zeros((h*size[0] , w*size[1] , 3))\n",
        "    for idx , image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        img[j*h:j*h +h , i*w : i*w+w , :] = image\n",
        "\n",
        "    return img\n",
        "\n",
        "def inverse_transform(image):\n",
        "    return (image + 1.)/2.\n",
        "\n",
        "def read_image_list(category):\n",
        "    filenames = []\n",
        "    print(\"list file\")\n",
        "    list = os.listdir(category)\n",
        "\n",
        "    for file in list:\n",
        "        filenames.append(category + \"/\" + file)\n",
        "\n",
        "    print(\"list file ending!\")\n",
        "\n",
        "    return filenames\n",
        "\n",
        "##from caffe\n",
        "def vis_square(visu_path , data , type):\n",
        "    \"\"\"Take an array of shape (n, height, width) or (n, height, width , 3)\n",
        "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
        "\n",
        "    # normalize data for display\n",
        "    data = (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "    # force the number of filters to be square\n",
        "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
        "\n",
        "    padding = (((0, n ** 2 - data.shape[0]) ,\n",
        "                (0, 1), (0, 1))  # add some space between filters\n",
        "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
        "    data = np.pad(data , padding, mode='constant' , constant_values=1)  # pad with ones (white)\n",
        "\n",
        "    # tilethe filters into an im age\n",
        "    data = data.reshape((n , n) + data.shape[1:]).transpose((0 , 2 , 1 , 3) + tuple(range(4 , data.ndim + 1)))\n",
        "\n",
        "    data = data.reshape((n * data.shape[1] , n * data.shape[3]) + data.shape[4:])\n",
        "\n",
        "    plt.imshow(data[:,:,0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    if type:\n",
        "        plt.savefig('./{}/weights.png'.format(visu_path) , format='png')\n",
        "    else:\n",
        "        plt.savefig('./{}/activation.png'.format(visu_path) , format='png')\n",
        "\n",
        "\n",
        "def sample_label():\n",
        "    num = 64\n",
        "    label_vector = np.zeros((num , 10), dtype=np.float)\n",
        "    for i in range(0 , num):\n",
        "        label_vector[i , int(i/8)] = 1.0\n",
        "    return label_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGu0nkhpmkwn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "构建网络"
      ]
    },
    {
      "metadata": {
        "id": "hapAKFHrmn5V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CGAN(object):\n",
        "\n",
        "    # build model\n",
        "    def __init__(self, data_ob, sample_dir, output_size, learn_rate, batch_size, z_dim, y_dim, log_dir\n",
        "         , model_path, visua_path):\n",
        "\n",
        "        self.data_ob = data_ob\n",
        "        self.sample_dir = sample_dir\n",
        "        self.output_size = output_size\n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.z_dim = z_dim\n",
        "        self.y_dim = y_dim\n",
        "        self.log_dir = log_dir\n",
        "        self.model_path = model_path\n",
        "        self.vi_path = visua_path\n",
        "        self.channel = self.data_ob.shape[2]\n",
        "        self.images = tf.placeholder(tf.float32, [batch_size, self.output_size, self.output_size, self.channel])\n",
        "        self.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim])\n",
        "        self.y = tf.placeholder(tf.float32, [self.batch_size, self.y_dim])\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        self.fake_images = self.gern_net(self.z, self.y)\n",
        "        G_image = tf.summary.image(\"G_out\", self.fake_images)\n",
        "        ##the loss of gerenate network\n",
        "        D_pro, D_logits = self.dis_net(self.images, self.y, False)\n",
        "        D_pro_sum = tf.summary.histogram(\"D_pro\", D_pro)\n",
        "\n",
        "        G_pro, G_logits = self.dis_net(self.fake_images, self.y, True)\n",
        "        G_pro_sum = tf.summary.histogram(\"G_pro\", G_pro)\n",
        "\n",
        "        D_fake_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(G_pro), logits=G_logits))\n",
        "\n",
        "        D_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D_pro), logits=D_logits))\n",
        "        G_fake_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(G_pro), logits=G_logits))\n",
        "\n",
        "        self.D_loss = D_real_loss + D_fake_loss\n",
        "        self.G_loss = G_fake_loss\n",
        "\n",
        "        loss_sum = tf.summary.scalar(\"D_loss\", self.D_loss)\n",
        "        G_loss_sum = tf.summary.scalar(\"G_loss\", self.G_loss)\n",
        "\n",
        "        self.merged_summary_op_d = tf.summary.merge([loss_sum, D_pro_sum])\n",
        "        self.merged_summary_op_g = tf.summary.merge([G_loss_sum, G_pro_sum, G_image])\n",
        "\n",
        "        t_vars = tf.trainable_variables()\n",
        "        self.d_var = [var for var in t_vars if 'dis' in var.name]\n",
        "        self.g_var = [var for var in t_vars if 'gen' in var.name]\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        opti_D = tf.train.AdamOptimizer(learning_rate=self.learn_rate, beta1=0.5).minimize(self.D_loss, var_list=self.d_var)\n",
        "        opti_G = tf.train.AdamOptimizer(learning_rate=self.learn_rate, beta1=0.5).minimize(self.G_loss,\n",
        "                                                                                         var_list=self.g_var)\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        with tf.Session(config=config) as sess:\n",
        "\n",
        "            sess.run(init)\n",
        "            summary_writer = tf.summary.FileWriter(self.log_dir, graph=sess.graph)\n",
        "\n",
        "            step = 0\n",
        "            while step <= 10000:\n",
        "\n",
        "                realbatch_array, real_labels = self.data_ob.getNext_batch(step)\n",
        "\n",
        "                # Get the z\n",
        "                batch_z = np.random.uniform(-1, 1, size=[self.batch_size, self.z_dim])\n",
        "                # batch_z = np.random.normal(0 , 0.2 , size=[batch_size , sample_size])\n",
        "\n",
        "                _, summary_str = sess.run([opti_D, self.merged_summary_op_d],\n",
        "                                          feed_dict={self.images: realbatch_array, self.z: batch_z, self.y: real_labels})\n",
        "                summary_writer.add_summary(summary_str, step)\n",
        "\n",
        "                _, summary_str = sess.run([opti_G, self.merged_summary_op_g],\n",
        "                                          feed_dict={self.z: batch_z, self.y: real_labels})\n",
        "                summary_writer.add_summary(summary_str, step)\n",
        "\n",
        "                if step % 50 == 0:\n",
        "\n",
        "                    D_loss = sess.run(self.D_loss, feed_dict={self.images: realbatch_array, self.z: batch_z, self.y: real_labels})\n",
        "                    fake_loss = sess.run(self.G_loss, feed_dict={self.z: batch_z, self.y: real_labels})\n",
        "                    print(\"Step %d: D: loss = %.7f G: loss=%.7f \" % (step, D_loss, fake_loss))\n",
        "\n",
        "                if np.mod(step, 50) == 1 and step != 0:\n",
        "\n",
        "                    sample_images = sess.run(self.fake_images, feed_dict={self.z: batch_z, self.y: sample_label()})\n",
        "                    save_images(sample_images, [8, 8],\n",
        "                                './{}/train_{:04d}.png'.format(self.sample_dir, step))\n",
        "\n",
        "                    self.saver.save(sess, self.model_path)\n",
        "\n",
        "                step = step + 1\n",
        "\n",
        "            save_path = self.saver.save(sess, self.model_path)\n",
        "            print (\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        init = tf.initialize_all_variables()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            self.saver.restore(sess, self.model_path)\n",
        "            \n",
        "            sample_z = np.random.uniform(1, -1, size=[self.batch_size, self.z_dim])\n",
        "\n",
        "            output = sess.run(self.fake_images, feed_dict={self.z: sample_z, self.y: sample_label()})\n",
        "\n",
        "            save_images(output, [8, 8], './{}/test{:02d}_{:04d}.png'.format(self.sample_dir, 0, 0))\n",
        "\n",
        "            image = cv2.imread('./{}/test{:02d}_{:04d}.png'.format(self.sample_dir, 0, 0), 0)\n",
        "\n",
        "            cv2.imshow(\"test\", image)\n",
        "\n",
        "            cv2.waitKey(-1)\n",
        "\n",
        "            print(\"Test finish!\")\n",
        "\n",
        "    def visual(self):\n",
        "\n",
        "        init = tf.initialize_all_variables()\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            self.saver.restore(sess, self.model_path)\n",
        "\n",
        "            realbatch_array, real_labels = self.data_ob.getNext_batch(0)\n",
        "            batch_z = np.random.uniform(-1, 1, size=[self.batch_size, self.z_dim])\n",
        "            # visualize the weights 1 or you can change weights_2 .\n",
        "            conv_weights = sess.run([tf.get_collection('weight_2')])\n",
        "            vis_square(self.vi_path, conv_weights[0][0].transpose(3, 0, 1, 2), type=1)\n",
        "\n",
        "            # visualize the activation 1\n",
        "            ac = sess.run([tf.get_collection('ac_2')],\n",
        "                          feed_dict={self.images: realbatch_array[:64], self.z: batch_z, self.y: sample_label()})\n",
        "\n",
        "            vis_square(self.vi_path, ac[0][0].transpose(3, 1, 2, 0), type=0)\n",
        "\n",
        "            print(\"the visualization finish!\")\n",
        "\n",
        "    def gern_net(self, z, y):\n",
        "\n",
        "        with tf.variable_scope('generator') as scope:\n",
        "\n",
        "            yb = tf.reshape(y, shape=[self.batch_size, 1, 1, self.y_dim])\n",
        "            z = tf.concat([z, y], 1)\n",
        "            c1, c2 = int( self.output_size / 4), int(self.output_size / 2 ) \n",
        "\n",
        "            # 10 stand for the num of labels\n",
        "            d1 = tf.nn.relu(batch_normal(fully_connect(z, output_size=1024, scope='gen_fully'), scope='gen_bn1'))\n",
        "\n",
        "            d1 = tf.concat([d1, y], 1)\n",
        "\n",
        "            d2 = tf.nn.relu(batch_normal(fully_connect(d1, output_size=7*7*2*64, scope='gen_fully2'), scope='gen_bn2'))\n",
        "\n",
        "            d2 = tf.reshape(d2, [self.batch_size, c1, c1, 64 * 2])\n",
        "            d2 = conv_cond_concat(d2, yb)\n",
        "\n",
        "            d3 = tf.nn.relu(batch_normal(de_conv(d2, output_shape=[self.batch_size, c2, c2, 128], name='gen_deconv1'), scope='gen_bn3'))\n",
        "\n",
        "            d3 = conv_cond_concat(d3, yb)\n",
        "\n",
        "            d4 = de_conv(d3, output_shape=[self.batch_size, self.output_size, self.output_size, self.channel], \n",
        "                         name='gen_deconv2', initializer = xavier_initializer())\n",
        "\n",
        "            return tf.nn.sigmoid(d4)\n",
        "\n",
        "    def dis_net(self, images, y, reuse=False):\n",
        "\n",
        "        with tf.variable_scope(\"discriminator\") as scope:\n",
        "\n",
        "            if reuse == True:\n",
        "                scope.reuse_variables()\n",
        "\n",
        "            # mnist data's shape is (28 , 28 , 1)\n",
        "            yb = tf.reshape(y, shape=[self.batch_size, 1, 1, self.y_dim])\n",
        "            # concat\n",
        "            concat_data = conv_cond_concat(images, yb)\n",
        "\n",
        "            conv1, w1 = conv2d(concat_data, output_dim=10, name='dis_conv1')\n",
        "            tf.add_to_collection('weight_1', w1)\n",
        "\n",
        "            conv1 = lrelu(conv1)\n",
        "            conv1 = conv_cond_concat(conv1, yb)\n",
        "            tf.add_to_collection('ac_1', conv1)\n",
        "\n",
        "\n",
        "            conv2, w2 = conv2d(conv1, output_dim=64, name='dis_conv2')\n",
        "            tf.add_to_collection('weight_2', w2)\n",
        "\n",
        "            conv2 = lrelu(batch_normal(conv2, scope='dis_bn1'))\n",
        "            tf.add_to_collection('ac_2', conv2)\n",
        "\n",
        "            conv2 = tf.reshape(conv2, [self.batch_size, -1])\n",
        "            conv2 = tf.concat([conv2, y], 1)\n",
        "\n",
        "            f1 = lrelu(batch_normal(fully_connect(conv2, output_size=1024, scope='dis_fully1'), scope='dis_bn2', reuse=reuse))\n",
        "            f1 = tf.concat([f1, y], 1)\n",
        "\n",
        "            out = fully_connect(f1, output_size=1, scope='dis_fully2',  initializer = xavier_initializer())\n",
        "\n",
        "            return tf.nn.sigmoid(out), out\n",
        "\n",
        "\n",
        "\n",
        "####Delete all flags before declare#####\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "go_qXmf72taA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "正式使用"
      ]
    },
    {
      "metadata": {
        "id": "e4vdb_JF2v9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del_all_flags(tf.flags.FLAGS)\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "flags.DEFINE_string(\"sample_dir\" , \"samples_for_test\" , \"the dir of sample images\")\n",
        "flags.DEFINE_integer(\"output_size\", 28 , \"the size of generate image\")\n",
        "flags.DEFINE_float(\"learn_rate\", 0.0002, \"the learning rate for gan\")\n",
        "flags.DEFINE_integer(\"batch_size\", 64, \"the batch number\")\n",
        "flags.DEFINE_integer(\"z_dim\", 100, \"the dimension of noise z\")\n",
        "flags.DEFINE_integer(\"y_dim\", 10, \"the dimension of condition y\")\n",
        "flags.DEFINE_string(\"log_dir\" , \"/tmp/tensorflow_mnist\" , \"the path of tensorflow's log\")\n",
        "flags.DEFINE_string(\"model_path\" , \"model/model.ckpt\" , \"the path of model\")\n",
        "flags.DEFINE_string(\"visua_path\" , \"visualization\" , \"the path of visuzation images\")\n",
        "flags.DEFINE_integer(\"op\" , 0, \"0: train ; 1:test ; 2:visualize\")\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "#\n",
        "if not os.path.exists(FLAGS.sample_dir):\n",
        "    os.makedirs(FLAGS.sample_dir)\n",
        "if not os.path.exists(FLAGS.log_dir):\n",
        "    os.makedirs(FLAGS.log_dir)\n",
        "if not os.path.exists(FLAGS.model_path):\n",
        "    os.makedirs(FLAGS.model_path)\n",
        "if not os.path.exists(FLAGS.visua_path):\n",
        "    os.makedirs(FLAGS.visua_path)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3ZwNPmW_LBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "859bb596-02f8-41b9-dc06-fd9b1760c0e3"
      },
      "cell_type": "code",
      "source": [
        "mn_object = Mnist()\n",
        "\n",
        "cg = CGAN(data_ob = mn_object, sample_dir = FLAGS.sample_dir, output_size=FLAGS.output_size, learn_rate=FLAGS.learn_rate\n",
        "     , batch_size=FLAGS.batch_size, z_dim=FLAGS.z_dim, y_dim=FLAGS.y_dim, log_dir=FLAGS.log_dir\n",
        "     , model_path=FLAGS.model_path, visua_path=FLAGS.visua_path)\n",
        "\n",
        "cg.build_model()\n",
        "print('building ending')\n",
        "#     if FLAGS.op == 0:\n",
        "\n",
        "#         cg.train()\n",
        "\n",
        "#     elif FLAGS.op == 1:\n",
        "\n",
        "#         cg.test()\n",
        "\n",
        "#     else:\n",
        "\n",
        "#         cg.visual()\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     tf.app.run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building ending\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l2My-zS4M10I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ]
    },
    {
      "metadata": {
        "id": "Xfkf-_3C4Ced",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3506
        },
        "outputId": "474d8ef8-da6d-4b3b-d141-f45f05540e18"
      },
      "cell_type": "code",
      "source": [
        "cg.train()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0: D: loss = 2.0477657 G: loss=1.2979581 \n",
            "Step 50: D: loss = 0.9174079 G: loss=1.1174006 \n",
            "Step 100: D: loss = 1.1272650 G: loss=0.9375610 \n",
            "Step 150: D: loss = 1.1656433 G: loss=0.8687811 \n",
            "Step 200: D: loss = 1.2950205 G: loss=0.8846235 \n",
            "Step 250: D: loss = 1.2470872 G: loss=0.7897882 \n",
            "Step 300: D: loss = 1.2718534 G: loss=0.7801040 \n",
            "Step 350: D: loss = 1.2777321 G: loss=0.7553237 \n",
            "Step 400: D: loss = 1.3269228 G: loss=0.7800381 \n",
            "Step 450: D: loss = 1.2404314 G: loss=0.8128428 \n",
            "Step 500: D: loss = 1.2379448 G: loss=0.8014950 \n",
            "Step 550: D: loss = 1.2435032 G: loss=0.8018104 \n",
            "Step 600: D: loss = 1.2059585 G: loss=0.7888486 \n",
            "Step 650: D: loss = 1.2256162 G: loss=0.7534531 \n",
            "Step 700: D: loss = 1.2180052 G: loss=0.8099821 \n",
            "Step 750: D: loss = 1.1797783 G: loss=0.7767166 \n",
            "Step 800: D: loss = 1.2781758 G: loss=0.8061879 \n",
            "Step 850: D: loss = 1.1945620 G: loss=0.7849449 \n",
            "Step 900: D: loss = 1.2442744 G: loss=0.7971402 \n",
            "Step 950: D: loss = 1.1758289 G: loss=0.7568796 \n",
            "Step 1000: D: loss = 1.2577736 G: loss=0.7326056 \n",
            "Step 1050: D: loss = 1.1966411 G: loss=0.7719443 \n",
            "Step 1100: D: loss = 1.2712975 G: loss=0.7492794 \n",
            "Step 1150: D: loss = 1.2537973 G: loss=0.8054880 \n",
            "Step 1200: D: loss = 1.2751082 G: loss=0.7569821 \n",
            "Step 1250: D: loss = 1.2282732 G: loss=0.7444597 \n",
            "Step 1300: D: loss = 1.2908597 G: loss=0.7567099 \n",
            "Step 1350: D: loss = 1.2828853 G: loss=0.7526231 \n",
            "Step 1400: D: loss = 1.3132057 G: loss=0.7076531 \n",
            "Step 1450: D: loss = 1.2966940 G: loss=0.7424864 \n",
            "Step 1500: D: loss = 1.3525631 G: loss=0.7925444 \n",
            "Step 1550: D: loss = 1.2889795 G: loss=0.7298274 \n",
            "Step 1600: D: loss = 1.2782793 G: loss=0.7542052 \n",
            "Step 1650: D: loss = 1.2590649 G: loss=0.7218487 \n",
            "Step 1700: D: loss = 1.2681915 G: loss=0.7343057 \n",
            "Step 1750: D: loss = 1.3148770 G: loss=0.6872218 \n",
            "Step 1800: D: loss = 1.2605456 G: loss=0.7606989 \n",
            "Step 1850: D: loss = 1.3004732 G: loss=0.7497432 \n",
            "Step 1900: D: loss = 1.3093686 G: loss=0.7236695 \n",
            "Step 1950: D: loss = 1.3008239 G: loss=0.7487460 \n",
            "Step 2000: D: loss = 1.2610714 G: loss=0.7252339 \n",
            "Step 2050: D: loss = 1.2744498 G: loss=0.7446427 \n",
            "Step 2100: D: loss = 1.3025525 G: loss=0.7519745 \n",
            "Step 2150: D: loss = 1.2800235 G: loss=0.7576367 \n",
            "Step 2200: D: loss = 1.2869825 G: loss=0.7571598 \n",
            "Step 2250: D: loss = 1.2717700 G: loss=0.7409325 \n",
            "Step 2300: D: loss = 1.3271577 G: loss=0.7306203 \n",
            "Step 2350: D: loss = 1.2569009 G: loss=0.7476467 \n",
            "Step 2400: D: loss = 1.2860264 G: loss=0.7358396 \n",
            "Step 2450: D: loss = 1.3097670 G: loss=0.7323586 \n",
            "Step 2500: D: loss = 1.2451290 G: loss=0.7696378 \n",
            "Step 2550: D: loss = 1.2792600 G: loss=0.7320138 \n",
            "Step 2600: D: loss = 1.3097010 G: loss=0.7504030 \n",
            "Step 2650: D: loss = 1.3645735 G: loss=0.7502738 \n",
            "Step 2700: D: loss = 1.2821641 G: loss=0.7442604 \n",
            "Step 2750: D: loss = 1.2667646 G: loss=0.7704407 \n",
            "Step 2800: D: loss = 1.2967093 G: loss=0.7157469 \n",
            "Step 2850: D: loss = 1.3619051 G: loss=0.7192263 \n",
            "Step 2900: D: loss = 1.2571383 G: loss=0.7365597 \n",
            "Step 2950: D: loss = 1.3081322 G: loss=0.7551316 \n",
            "Step 3000: D: loss = 1.2671025 G: loss=0.7473049 \n",
            "Step 3050: D: loss = 1.3014686 G: loss=0.7364733 \n",
            "Step 3100: D: loss = 1.3072654 G: loss=0.6952952 \n",
            "Step 3150: D: loss = 1.2686688 G: loss=0.7328812 \n",
            "Step 3200: D: loss = 1.2746325 G: loss=0.7293732 \n",
            "Step 3250: D: loss = 1.3157907 G: loss=0.7022586 \n",
            "Step 3300: D: loss = 1.3141840 G: loss=0.7173555 \n",
            "Step 3350: D: loss = 1.2835836 G: loss=0.7360009 \n",
            "Step 3400: D: loss = 1.3112346 G: loss=0.7035800 \n",
            "Step 3450: D: loss = 1.3412192 G: loss=0.7064893 \n",
            "Step 3500: D: loss = 1.2920706 G: loss=0.7382802 \n",
            "Step 3550: D: loss = 1.2936493 G: loss=0.7280084 \n",
            "Step 3600: D: loss = 1.2575322 G: loss=0.7179071 \n",
            "Step 3650: D: loss = 1.3314948 G: loss=0.6700885 \n",
            "Step 3700: D: loss = 1.2886848 G: loss=0.7559062 \n",
            "Step 3750: D: loss = 1.2872819 G: loss=0.7444202 \n",
            "Step 3800: D: loss = 1.2973795 G: loss=0.7060480 \n",
            "Step 3850: D: loss = 1.2563039 G: loss=0.7929888 \n",
            "Step 3900: D: loss = 1.2483065 G: loss=0.7524338 \n",
            "Step 3950: D: loss = 1.3344294 G: loss=0.7577742 \n",
            "Step 4000: D: loss = 1.2884833 G: loss=0.7135279 \n",
            "Step 4050: D: loss = 1.3345346 G: loss=0.6969892 \n",
            "Step 4100: D: loss = 1.2633098 G: loss=0.7489123 \n",
            "Step 4150: D: loss = 1.3113014 G: loss=0.7091417 \n",
            "Step 4200: D: loss = 1.2602437 G: loss=0.7538083 \n",
            "Step 4250: D: loss = 1.2630284 G: loss=0.7453279 \n",
            "Step 4300: D: loss = 1.3130735 G: loss=0.7234456 \n",
            "Step 4350: D: loss = 1.2525859 G: loss=0.7328926 \n",
            "Step 4400: D: loss = 1.3275391 G: loss=0.7183153 \n",
            "Step 4450: D: loss = 1.2972043 G: loss=0.7219343 \n",
            "Step 4500: D: loss = 1.2832668 G: loss=0.7434169 \n",
            "Step 4550: D: loss = 1.2712146 G: loss=0.7412311 \n",
            "Step 4600: D: loss = 1.2702383 G: loss=0.7291391 \n",
            "Step 4650: D: loss = 1.2803068 G: loss=0.7317947 \n",
            "Step 4700: D: loss = 1.2870630 G: loss=0.7331564 \n",
            "Step 4750: D: loss = 1.3084650 G: loss=0.7082793 \n",
            "Step 4800: D: loss = 1.3239729 G: loss=0.7498178 \n",
            "Step 4850: D: loss = 1.2857270 G: loss=0.7224603 \n",
            "Step 4900: D: loss = 1.2990174 G: loss=0.7259343 \n",
            "Step 4950: D: loss = 1.2855273 G: loss=0.7195958 \n",
            "Step 5000: D: loss = 1.2527155 G: loss=0.7437176 \n",
            "Step 5050: D: loss = 1.3243878 G: loss=0.7019194 \n",
            "Step 5100: D: loss = 1.3719298 G: loss=0.7198586 \n",
            "Step 5150: D: loss = 1.2887261 G: loss=0.7190516 \n",
            "Step 5200: D: loss = 1.3238432 G: loss=0.7282922 \n",
            "Step 5250: D: loss = 1.3145034 G: loss=0.7198457 \n",
            "Step 5300: D: loss = 1.3005170 G: loss=0.7484686 \n",
            "Step 5350: D: loss = 1.2754834 G: loss=0.7373929 \n",
            "Step 5400: D: loss = 1.3062068 G: loss=0.6987677 \n",
            "Step 5450: D: loss = 1.3025562 G: loss=0.7466291 \n",
            "Step 5500: D: loss = 1.3009024 G: loss=0.7213525 \n",
            "Step 5550: D: loss = 1.2793524 G: loss=0.7125819 \n",
            "Step 5600: D: loss = 1.2948469 G: loss=0.7173122 \n",
            "Step 5650: D: loss = 1.3129979 G: loss=0.7230036 \n",
            "Step 5700: D: loss = 1.2930791 G: loss=0.7140882 \n",
            "Step 5750: D: loss = 1.2905922 G: loss=0.7309153 \n",
            "Step 5800: D: loss = 1.3615670 G: loss=0.7018199 \n",
            "Step 5850: D: loss = 1.3322256 G: loss=0.7138040 \n",
            "Step 5900: D: loss = 1.2847599 G: loss=0.7230955 \n",
            "Step 5950: D: loss = 1.3119091 G: loss=0.7197106 \n",
            "Step 6000: D: loss = 1.2637788 G: loss=0.7529493 \n",
            "Step 6050: D: loss = 1.3438343 G: loss=0.6986493 \n",
            "Step 6100: D: loss = 1.2797576 G: loss=0.7254627 \n",
            "Step 6150: D: loss = 1.3321129 G: loss=0.7120011 \n",
            "Step 6200: D: loss = 1.2898400 G: loss=0.7416933 \n",
            "Step 6250: D: loss = 1.3212490 G: loss=0.6960453 \n",
            "Step 6300: D: loss = 1.3098851 G: loss=0.7135642 \n",
            "Step 6350: D: loss = 1.3062599 G: loss=0.7121247 \n",
            "Step 6400: D: loss = 1.2742279 G: loss=0.7533448 \n",
            "Step 6450: D: loss = 1.3084025 G: loss=0.7452599 \n",
            "Step 6500: D: loss = 1.3185366 G: loss=0.7356303 \n",
            "Step 6550: D: loss = 1.2743618 G: loss=0.7431329 \n",
            "Step 6600: D: loss = 1.3083568 G: loss=0.7095287 \n",
            "Step 6650: D: loss = 1.2650393 G: loss=0.7358477 \n",
            "Step 6700: D: loss = 1.3133602 G: loss=0.7177158 \n",
            "Step 6750: D: loss = 1.3097796 G: loss=0.7074528 \n",
            "Step 6800: D: loss = 1.3391578 G: loss=0.6956637 \n",
            "Step 6850: D: loss = 1.2748027 G: loss=0.7630175 \n",
            "Step 6900: D: loss = 1.2943370 G: loss=0.7339016 \n",
            "Step 6950: D: loss = 1.3188725 G: loss=0.7488189 \n",
            "Step 7000: D: loss = 1.2619905 G: loss=0.7414941 \n",
            "Step 7050: D: loss = 1.3377476 G: loss=0.7189197 \n",
            "Step 7100: D: loss = 1.2709932 G: loss=0.7453781 \n",
            "Step 7150: D: loss = 1.2918828 G: loss=0.7361017 \n",
            "Step 7200: D: loss = 1.2277441 G: loss=0.7661991 \n",
            "Step 7250: D: loss = 1.2910094 G: loss=0.7289447 \n",
            "Step 7300: D: loss = 1.2927983 G: loss=0.7146173 \n",
            "Step 7350: D: loss = 1.2974213 G: loss=0.7199066 \n",
            "Step 7400: D: loss = 1.2981459 G: loss=0.7315890 \n",
            "Step 7450: D: loss = 1.3008475 G: loss=0.7247154 \n",
            "Step 7500: D: loss = 1.2876557 G: loss=0.7311783 \n",
            "Step 7550: D: loss = 1.2863717 G: loss=0.7188874 \n",
            "Step 7600: D: loss = 1.2841731 G: loss=0.7449813 \n",
            "Step 7650: D: loss = 1.2726599 G: loss=0.7718141 \n",
            "Step 7700: D: loss = 1.3900876 G: loss=0.7219753 \n",
            "Step 7750: D: loss = 1.3111207 G: loss=0.7250979 \n",
            "Step 7800: D: loss = 1.2837430 G: loss=0.7160486 \n",
            "Step 7850: D: loss = 1.3282279 G: loss=0.6810367 \n",
            "Step 7900: D: loss = 1.2929400 G: loss=0.7255631 \n",
            "Step 7950: D: loss = 1.2512841 G: loss=0.7611498 \n",
            "Step 8000: D: loss = 1.3058434 G: loss=0.7236042 \n",
            "Step 8050: D: loss = 1.2870495 G: loss=0.7128142 \n",
            "Step 8100: D: loss = 1.3272268 G: loss=0.7202718 \n",
            "Step 8150: D: loss = 1.2998798 G: loss=0.7054586 \n",
            "Step 8200: D: loss = 1.2469273 G: loss=0.7607462 \n",
            "Step 8250: D: loss = 1.2084494 G: loss=0.7613893 \n",
            "Step 8300: D: loss = 1.2558737 G: loss=0.7330999 \n",
            "Step 8350: D: loss = 1.3323632 G: loss=0.7183146 \n",
            "Step 8400: D: loss = 1.2873628 G: loss=0.7350604 \n",
            "Step 8450: D: loss = 1.2793127 G: loss=0.7494367 \n",
            "Step 8500: D: loss = 1.2118940 G: loss=0.7794634 \n",
            "Step 8550: D: loss = 1.3104529 G: loss=0.7128004 \n",
            "Step 8600: D: loss = 1.3357788 G: loss=0.7146304 \n",
            "Step 8650: D: loss = 1.2805068 G: loss=0.7185448 \n",
            "Step 8700: D: loss = 1.3015707 G: loss=0.7262498 \n",
            "Step 8750: D: loss = 1.3274275 G: loss=0.7108665 \n",
            "Step 8800: D: loss = 1.2628126 G: loss=0.7613530 \n",
            "Step 8850: D: loss = 1.3159287 G: loss=0.7135198 \n",
            "Step 8900: D: loss = 1.2715644 G: loss=0.7700385 \n",
            "Step 8950: D: loss = 1.2552924 G: loss=0.7566119 \n",
            "Step 9000: D: loss = 1.3111943 G: loss=0.7283870 \n",
            "Step 9050: D: loss = 1.2786701 G: loss=0.7257724 \n",
            "Step 9100: D: loss = 1.3193285 G: loss=0.6825419 \n",
            "Step 9150: D: loss = 1.2792916 G: loss=0.7098942 \n",
            "Step 9200: D: loss = 1.2824144 G: loss=0.7558042 \n",
            "Step 9250: D: loss = 1.2736630 G: loss=0.7375284 \n",
            "Step 9300: D: loss = 1.2647264 G: loss=0.7453485 \n",
            "Step 9350: D: loss = 1.2984587 G: loss=0.7566086 \n",
            "Step 9400: D: loss = 1.2892320 G: loss=0.7165809 \n",
            "Step 9450: D: loss = 1.2947476 G: loss=0.7424302 \n",
            "Step 9500: D: loss = 1.2625681 G: loss=0.7520784 \n",
            "Step 9550: D: loss = 1.2713463 G: loss=0.7498173 \n",
            "Step 9600: D: loss = 1.2868041 G: loss=0.7298380 \n",
            "Step 9650: D: loss = 1.2710284 G: loss=0.7621239 \n",
            "Step 9700: D: loss = 1.2831731 G: loss=0.7467608 \n",
            "Step 9750: D: loss = 1.2932851 G: loss=0.7353712 \n",
            "Step 9800: D: loss = 1.2685267 G: loss=0.7163354 \n",
            "Step 9850: D: loss = 1.3025179 G: loss=0.7071016 \n",
            "Step 9900: D: loss = 1.3094141 G: loss=0.7319795 \n",
            "Step 9950: D: loss = 1.2922628 G: loss=0.7214697 \n",
            "Step 10000: D: loss = 1.2815151 G: loss=0.7344118 \n",
            "Model saved in file: model/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BE8xk6Ca4ELY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cg.test()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lTs-fBRH86f1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cg.visual()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}