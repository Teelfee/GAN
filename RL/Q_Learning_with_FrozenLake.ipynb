{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q Learning with FrozenLake.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teelfee/GT-and-ML-exploration/blob/development/RL/Q_Learning_with_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pzMZOjhCPAGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q* Learning with FrozenLake üïπÔ∏è‚õÑ\n",
        "<br> \n",
        "In this Notebook, we'll implement an agent <b>that plays FrozenLake.</b>\n",
        "<img src=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/frozenlake.png?raw=1\" alt=\"Frozen Lake\"/>\n",
        "\n",
        "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, <b>so you won't always move in the direction you intend (stochastic environment)</b>"
      ]
    },
    {
      "metadata": {
        "id": "Q-2Yo8mHPAGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
        "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
        "<br>\n",
        "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
        "<br><br>\n",
        "    \n",
        "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
        "<br>\n",
        "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
        "<br>\n",
        "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
        "<br><br>\n",
        "</p> \n",
        "\n",
        "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
        "\n",
        "\n",
        "## Any questions üë®‚Äçüíª\n",
        "<p> If you have any questions, feel free to ask me: </p>\n",
        "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
        "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
        "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
        "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
        "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
        "    \n",
        "## How to help  üôå\n",
        "3 ways:\n",
        "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
        "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
        "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "pSOH1zfWPAGO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving on the notebook **you need to understand**:\n",
        "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
        "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
        "- In the [video version](https://www.youtube.com/watch?v=q2ZOEFAaaI0)  we implemented a Q-learning agent that learns to play OpenAI Taxi-v2 üöï with Numpy."
      ]
    },
    {
      "metadata": {
        "id": "sJV2jkBHPAGP",
        "colab_type": "code",
        "outputId": "2ce1e6ce-53e8-447b-b6c9-886b7a9dfc96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/q2ZOEFAaaI0?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/q2ZOEFAaaI0?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "HZn5vy7Sqi2u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-94Q9e7TPAGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 0: Import the dependencies üìö\n",
        "We use 3 libraries:\n",
        "- `Numpy` for our Qtable\n",
        "- `OpenAI Gym` for our FrozenLake Environment\n",
        "- `Random` to generate random numbers"
      ]
    },
    {
      "metadata": {
        "id": "WzJgf8TlPAGW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v-puY7CVPAGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Create the environment üéÆ\n",
        "- Here we'll create the FrozenLake environment. \n",
        "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
        "- In our case we choose to use Frozen Lake."
      ]
    },
    {
      "metadata": {
        "id": "4gRjxU4oPAGa",
        "colab_type": "code",
        "outputId": "2f85d297-b3ec-4674-dd52-0a5d2bbca497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "N2dMG6iPPAGc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Create the Q-table and initialize it üóÑÔ∏è\n",
        "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
        "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
      ]
    },
    {
      "metadata": {
        "id": "TpZ4yb9EPAGd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6dCOvJkvPAGg",
        "colab_type": "code",
        "outputId": "9439b489-4ea6-4d34-f4e0-a1858072f5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYD-rtcbPAGk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create the hyperparameters ‚öôÔ∏è\n",
        "- Here, we'll specify the hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "MaB-ov5tPAGl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_episodes = 15000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.005             # Exponential decay rate for exploration prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Jolnw_hPAGo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: The Q learning algorithm üß†\n",
        "- Now we implement the Q learning algorithm:\n",
        "<img src=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/qtable_algo.png?raw=1\" alt=\"Q algo\"/>"
      ]
    },
    {
      "metadata": {
        "id": "GOdG07JlPAGq",
        "colab_type": "code",
        "outputId": "e378d8c3-2b32-4cb3-e65d-f8c4bf9f5acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.4604666666666667\n",
            "[[2.58332235e-01 6.90175812e-02 8.94599869e-02 9.66771164e-02]\n",
            " [1.45934912e-02 3.80451566e-04 1.58978124e-03 1.78965204e-01]\n",
            " [1.23542158e-03 3.31374080e-02 2.29055117e-03 2.31691418e-03]\n",
            " [3.40279166e-04 1.45445968e-03 5.25400625e-05 2.31356034e-02]\n",
            " [2.16269824e-01 2.98801248e-03 9.12035347e-02 4.84163545e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.19373635e-02 1.65261611e-08 4.38233105e-03 1.40721929e-08]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.44132114e-03 6.00198202e-02 7.35769091e-03 3.19749591e-01]\n",
            " [9.31413864e-04 3.61766781e-01 4.97485667e-03 7.19526736e-03]\n",
            " [7.86144347e-02 1.29362711e-03 8.11750431e-03 6.96237690e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.74760715e-02 1.65618315e-01 7.43760765e-01 1.19064199e-01]\n",
            " [2.32635576e-01 3.43456985e-01 2.10434490e-01 9.75760663e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YDGUIoUUPAGt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5: Use our Q-table to play FrozenLake ! üëæ\n",
        "- After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n",
        "- By running this cell you can see our agent playing FrozenLake."
      ]
    },
    {
      "metadata": {
        "id": "HArmiKP0PAGu",
        "colab_type": "code",
        "outputId": "6886ca3e-e016-481d-8e3a-fee2367626f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        }
      },
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 25\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 72\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 12\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 23\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of steps 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fFYeuWJAteR8",
        "colab_type": "code",
        "outputId": "ec0298b1-0b6c-4dba-d3fe-5fd65cd781d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "A simple example for Reinforcement Learning using table lookup Q-learning method.\n",
        "An agent \"o\" is on the left of a 1 dimensional world, the treasure is on the rightmost location.\n",
        "Run this program and to see how the agent will improve its strategy of finding the treasure.\n",
        "\n",
        "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "np.random.seed(2)  # reproducible\n",
        "\n",
        "\n",
        "N_STATES = 6   # the length of the 1 dimensional world\n",
        "ACTIONS = ['left', 'right']     # available actions\n",
        "EPSILON = 0.9   # greedy police\n",
        "ALPHA = 0.1     # learning rate\n",
        "GAMMA = 0.9    # discount factor\n",
        "MAX_EPISODES = 13   # maximum episodes\n",
        "FRESH_TIME = 0.2   # fresh time for one move\n",
        "\n",
        "\n",
        "\n",
        "def build_q_table(n_states, actions):\n",
        "    table = pd.DataFrame(\n",
        "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
        "        columns=actions,    # actions's name\n",
        "    )\n",
        "    # print(table)    # show table\n",
        "    return table\n",
        "\n",
        "\n",
        "def choose_action(state, q_table):\n",
        "    # This is how to choose an action\n",
        "    state_actions = q_table.iloc[state, :]\n",
        "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
        "        action_name = np.random.choice(ACTIONS)\n",
        "    else:   # act greedy\n",
        "        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
        "    return action_name\n",
        "\n",
        "\n",
        "def get_env_feedback(S, A):\n",
        "    # This is how agent will interact with the environment\n",
        "    if A == 'right':    # move right\n",
        "        if S == N_STATES - 2:   # terminate\n",
        "            S_ = 'terminal'\n",
        "            R = 1\n",
        "        else:\n",
        "            S_ = S + 1\n",
        "            R = 0\n",
        "    else:   # move left\n",
        "        R = 0\n",
        "        if S == 0:\n",
        "            S_ = S  # reach the wall\n",
        "        else:\n",
        "            S_ = S - 1\n",
        "    return S_, R\n",
        "\n",
        "\n",
        "def update_env(S, episode, step_counter):\n",
        "    # This is how environment be updated\n",
        "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
        "    if S == 'terminal':\n",
        "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
        "        print('\\r{}'.format(interaction), end='')\n",
        "        time.sleep(2)\n",
        "        print('\\r                                ', end='')\n",
        "    else:\n",
        "        env_list[S] = 'o'\n",
        "        interaction = ''.join(env_list)\n",
        "        print('\\r{}'.format(interaction), end='')\n",
        "        time.sleep(FRESH_TIME)\n",
        "\n",
        "\n",
        "def rl():\n",
        "    # main part of RL loop\n",
        "    q_table = build_q_table(N_STATES, ACTIONS)\n",
        "    for episode in range(MAX_EPISODES):\n",
        "        step_counter = 0\n",
        "        S = 0\n",
        "        is_terminated = False\n",
        "        update_env(S, episode, step_counter)\n",
        "        while not is_terminated:\n",
        "\n",
        "            A = choose_action(S, q_table)\n",
        "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
        "            q_predict = q_table.loc[S, A]\n",
        "            if S_ != 'terminal':\n",
        "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
        "            else:\n",
        "                q_target = R     # next state is terminal\n",
        "                is_terminated = True    # terminate this episode\n",
        "\n",
        "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n",
        "            S = S_  # move to next state\n",
        "\n",
        "            update_env(S, episode, step_counter+1)\n",
        "            step_counter += 1\n",
        "    return q_table\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    q_table = rl()\n",
        "    print('\\r\\nQ-table:\\n')\n",
        "    print(q_table)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---o-T"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lyPoPxPbxQJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}